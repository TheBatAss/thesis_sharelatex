\chapter{Related Work}\label{related_ch}
With the popularity of smartphones and smartwatches a lot of new possibilities has been made possible on the world of self tracking. This has in turn created a lot of research related to the topic. This Chapter will go through the most relevant work that related to this thesis, describing the work that has been done by others and how it relates to this thesis.


\section{Reducing the Burden of Self Tracking}
Self tracking can solve the problem of memory recall bias, by letting the user register observation in the moment (or very close to) they occur. But requiring the user to self register observations introduces a burden of the work involved in doing so. As mentioned traditionally this has been done with pen and paper, but with the invention of smartphones and smartwatches this burden can be reduces. The work of Ponnada et al., 2017\cite{compare} investigate the interruption burden with both smartphones and smartwatches when used with both regular EMA and $\mu$EMA. The difference between EMA and $\mu$EMA is that EMA will prompt the user less than $\mu$EMA, but requires the user to answer several questions back to back, where $\mu$EMA will instead prompt the user much more frequently, but only require the user to answer one question, which can be answered on the same screen showing the question. 

In a prior 4 week pilot study Ponnada et al.\cite{compare} found that smartwatch $\mu$EMA demonstrated higher response rates and participants reported a lower perceived burden than smartphone EMA, even though the interruption rate for smartwatch $\mu$EMA was 8 times higher than smartphone EMA. In a new 4 week study they gathered data based on a smartwatch EMA in order to determine if the previous result where due to the fact that the data was gathered on a watch, or if it in fact was due to the $\mu$EMA. Their results showed that there were no statistically significant differences in compliance, completion, and first-prompt response rates observed between smartphone EMA and smartwatch EMA. But smartwatch $\mu$EMA response rates where significantly higher than smartwatch EMA. Their results suggest that the higher compliance and lower perceived burden is more likely to do with $\mu$EMA than the device itself and that compliance with EMA may not improve simply by changing the device.

The work of Ponnada et al. only looked into EMA where the device prompts the user for information, not situations where the user registers an observation. In his thesis Kami\'nski\cite{tomas} investigates how to enable self logging with different forms of input: a binary sample, selection input form a list, VAS and a numerical scale. He designed and implemented all of the input methods onto a smartwatch, where on the watch face each method input method could be selected. In Figure \ref{tomas_design} we can see his design, the watch face has a shortcut for each input method. In Figure \ref{tomas_design2} we see how the interaction occurs, and that it takes 1-3 touches to log the desired data depending on the input method.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/tomas_design.png}
    \caption{Design for self logging on a smartwatch by Kami\'nski\cite[p.23]{tomas}}
    \label{tomas_design}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/tomas_design2.png}
    \caption{Interaction for self logging on a smartwatch by Kami\'nski\cite[p.23]{tomas}}
    \label{tomas_design2}
\end{figure}

For comparison Kami\'nski also implemented a similar design on a smartphone following the \emph{Android material design guide}\cite{android_design} and conducted an experiment where he compared the two devices performance (interaction time, lower is better). His result showed a reduction in the user interaction time by approximately 30\% on average, thus successfully reducing the burden related to self logging. Beside his experiment Kami\'nski also had himself and to others use his design to collect data about their daily life (sneezing, skin itching and water consumption) over a 13-21 week period. What he found was a high compliance rate, all though it is a fairly biased test, since all three participants where involved in the project and are working with self logging technologies on a daily basis. 

Even though Kami\'nski's design was a success, there persist an underlying problem with smartwatches; that they still require the user to user to activate them (often by tapping the screen several times) and then the user has to look at the screen to touch the right section to register the input. Dam-Jensen\cite{dam} investigates this issue in his thesis. He introduces three ways of performing self logging, all using a \say{smartbutton} (wireless button connected to smartphone/smartwatch) to achieve a display-less logging system. The three input method that Dam-Jensen designed was:

\begin{enumerate}
	\item Hold Button: Measure the time the button is held down.
	\item Rotate Lower Arm: Using the built in gyroscope in the smartbutton, the difference in rotation from when the button was pressed to it was released is measured. In order to achieve a measurement the user would hold the button in the hand, rotate the lower arm to a neutral position, hold down the button, rotate the lower arm to the end position and release the button. 
	\item Rotate Upper Arm Around Elbow: Similar to the previous method, but instead of rotating the lower arm, the user would raise/lower his/hers upper arm around the elbow.
\end{enumerate}

For a comparison baseline method a standard VAS was used. In order to compare his three methods to the VAS Dam-Jensen conducted an experiment based on the work of Matejka et al.\cite{grey}, where participants are asked to rate a color between white and black on a grey scale where the colors are equally spaced out on the CIELAB color space\cite{cielab} (more on this later). His results showed that there where no significant difference in accuracy between his three designed input methods compared and to the baseline, thus meaning that all three methods are viable and could be used for self logging, and in their simple nature can reduce the burden on the user.








\section{Long Term Use of Self Logging}
As mentioned Ponnada et al.\cite{compare} conducted two four week studies with EMA and $\mu$EMA and Kami\'nski conducted a even longer 13-21 week use case where three people used his design to track daily activities. Both Ponnada et al. and Kami\'nski's work show great potential for real use, but none of them utilizes a device similar to that of Dam-Jensen's work, where there is no screen and only a single button for registering observations.

In a case study Larsen et al.\cite{eg} investigated the real life use case of a novel device for self logging. They had a Danish veteran suffering from PTSD equipped with a \say{smartbutton}, that when pressed will store a time stamp. By connecting a smartphone the data can be transferred from the smartbutton to a data file for further analysis. The veteran was then instructed to press the button whenever he experienced a specific \say{trigger event} related to his PTSD (the \say{bodily sensation} was chosen after a two-hour assessment interview together with his therapist). The veteran wore the smartbutton for 100 days, pressing the button whenever he experienced the specific bodily sensation.

The results showed that the veteran had a high compliance, actively using the device throughout the 100 days (due to a technical issue no data was collected during the second week). The data collected revealed patterns that explained why/when the veteran's events would be triggered, which would not have revealed itself from the conversational assessment. In other words, not only was it a success from a technical and usability standpoint, but it also succeeded in bringing valuable new information to the treatment process. 







\section{Comparing Ratings of Behavior}
When developing new methods for self logging it is important to access the feasibility of these methods. As mentioned Kami\'nski did this simply by comparing the user interaction time, while Dam-Jensen did a more comprehensive comparison by looking into how precise the data recorded with his designs compared to the VAS. He did so by having his participants rate shades of grey with the different input methods. That experiment was inspired by the work of Matejka et al.\cite{grey} where they investigate the bias of different slider designs. In order to do so they base their experiment on the work of Borg and Borg\cite{borg} that extensively studied using a \say{scale of blackness} as a stimulus and concluded that it serves as a good test of general rating behaviour, and further more it provides a truth value (the color displayed) which can be compared to the response rating from the participant. Matejka et al. designed their experiment to test severely different slider designs with various combinations of labels, tick marks and bands. In Figure \ref{mate_ex} a design with 2 labels and 5 ticks is shown. Matejka et al. recruited participants via \emph{Amazon Mechanical Turk}\cite{turk}. Participants where then showed one of the 50 shades of grey (as seen in Figure \ref{50shades}), then asked to rate the color from white to blakc using the slider. This process was repeated 50 times so each participants was exposed to all the shades of grey (in random order), this was then repeated until the participants had been exposed to each shade of grey 4 times, 200 trials in total. Each participant was only exposed to one slider design, and was afterwards excluded to participate further. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/mate_ex.png}
    \caption{Slider bias experiment by Matejka et al.\cite{grey} with 2 labels and 5 ticks}
    \label{mate_ex}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/50shades.png}
    \caption{50 shades of grey used by Matejka et al.\cite{grey}}
    \label{50shades}
\end{figure}

With their results they investigated the bias for each slider design by looking at the distribution of responses and calculating a \say{smoothness} value representing the bias as well as precision and response time. Their results showed that sliders with ticks introduces the most bias towards the tick marks, followed by sliders with many labels and combination of the two. Banded sliders showed only small amount of bias and sliders with only two labels showed the least amount of bias. The higher amount of both ticks and labels resulted in higher precision and the average reaction dime only differed by one second between the fastest and slowest designs.







\section{Authors Previous Work}
In a previous project the author created an app for self tracking a person's mood\cite{mood}. The app's main screen had 11 buttons, labeled from 0-10, with the added text on \say{Very bad mood} on 0, \say{Neutral bad mood} on 5 and \say{Very good mood} on 10 and a button labeled \say{Mood history} as seen on the left in Figure \ref{mood}. When one of the buttons from 0-10 is pressed the \say{mood value} and a time stamp is saved. Pressing the \say{Mood history} will take the user to the second screen (seen on the right in Figure \ref{mood}) and a scroolable list of all recorded moods are shown, with the latest entry on top together with a button for sharing the mood history (export to a CSV file and send via email) and a button to delete the mood history.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.8\textwidth]{figures/mood.png}
    \caption{Mood Tracker app. Register a mood on the left \& mood history on the right\cite{mood}}
    \label{mood}
\end{figure}

The focus off that small project was on the implementation of the app, so no investigation went in to the actual use of the app (beside the author testing it on himself), but it was prioritized that the app was as quick and easy to use as possible. With the limitation of being a smartphone app (not a widget similar) this design seems to one of the most simple and efective solutions, it enables the user to quickly register events, since the first screen seen when the app is opened is the main screen. But it does illustrate the limitation of smartphone apps, comparing it to the designs of Dam-Jensen it is quite clear that the time and burden of getting your phone out of your pocket, turning it on/unlocking it, finding and opening the app before finally being apple to register a input is much greater than the time and burden involved with for example rotating your lower arm and pressing a button. 



\section{Relation to This Thesis}
The work of Ponnada et al.\cite{compare}, Kami\'nski\cite{tomas} and Dam-Jensen\cite{dam} all point towards the benefit reducing the burdens. Their work has been a great inspiration for this thesis, together with Larsen et al. work showing the real world benefit of in situ measurements. This thesis will further investigate two of Dam-Jensen's designs, the \say{Rotate Lower Arm} and \say{Rotate Upper Arm Around Elbow} methods, developing these methods a bit further and then comparing them to a VAS by a \say{rating of blackness} experiment. And the authors previous work has shown that even a well designed smartphone app still creates a lot of burden for the user when comparing it to the more novel designs of Dam-Jensen\cite{dam}.